{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "76884070d6fffa8833641bcf58e4f593836190bc88ca5bce17630bae63141ea0"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 問題1 ブレンディングのスクラッチ実装"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor, ARDRegression, RidgeCV\n",
    "from sklearn.linear_model import TheilSenRegressor, RANSACRegressor, HuberRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "x_df = train.loc[:,[\"GrLivArea\", \"YearBuilt\"]]\n",
    "y_df = train.loc[:, 'SalePrice']\n",
    "# 数値を底を０に調整\n",
    "x_df = x_df.apply(np.log1p)\n",
    "y_df = y_df.apply(np.log1p)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_df.values, y_df.values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['mse : LinearRegression : 0.05207835114334287'\n",
      " 'mse : SVR : 0.05393751032686638'\n",
      " 'mse : DecisionTreeRegressor : 0.06602628200076659']\n",
      "------------------------------------\n",
      "['score :LinearRegression : 0.7209255921255862'\n",
      " 'score :SVR : 0.7109628391409917'\n",
      " 'score :DecisionTreeRegressor : 0.6461822398563322']\n",
      "mse : blending_Data : 0.047431510278425515\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# 学習に使いたいデータセットをリストに格納\n",
    "data_set = []\n",
    "data_set.append((x_train, x_test, y_train, y_test))\n",
    "# SVRのハイパーパラメーター（変更しやすいように別で辞書で用意）\n",
    "\n",
    "def blending(data_set, c=10, g=1, ep=0, Kernel=\"rbf\"):\n",
    "    \"\"\"\n",
    "    訓練用のデータセットをリストで入力、MSEを返す\n",
    "    ----------\n",
    "    Parameter\n",
    "    data_set : list\n",
    "      学習・検証するデータ（4つ）\n",
    "    c\n",
    "      SVMのハイパーパラメーター\n",
    "    ----------\n",
    "    Return\n",
    "    mse_list : list\n",
    "      学習したそれぞれのmseをリストで返す\n",
    "    \"\"\"\n",
    "    learn_set = [LinearRegression(), \n",
    "                 SVR(C=c, gamma=g, kernel=Kernel, epsilon=ep), \n",
    "                 DecisionTreeRegressor(random_state=0)]\n",
    "    clf_list = []\n",
    "    for learn in learn_set:\n",
    "        clf = learn\n",
    "        clf_list.append((clf))\n",
    "\n",
    "    pred_list = np.array([])\n",
    "    mse_list = np.array([])\n",
    "    score_list = np.array([])\n",
    "\n",
    "    for x_train, x_test, y_train, y_test in data_set:\n",
    "        for i, model in enumerate(clf_list):\n",
    "            model.fit(x_train, y_train)\n",
    "            pred = model.predict(x_test)\n",
    "            score = model.score(x_test, y_test)\n",
    "            pred_list = np.append(pred_list, pred)\n",
    "            score_list = np.append(score_list, f'score :{model.__class__.__name__} : {score}')\n",
    "\n",
    "            mse = mean_squared_error(y_test, pred)\n",
    "            mse_list = np.append(mse_list, f'mse : {model.__class__.__name__} : {mse}')\n",
    "    return mse_list, pred_list, score_list\n",
    "\n",
    "# MSEの結果\n",
    "# SVMのカーネルをrbfとlinearで比較 \n",
    "# linearの場合は半分以上mseの数値が下がった\n",
    "print(blending(data_set, c=100, g=10, ep=0, Kernel=\"linear\")[0])\n",
    "print('------------------------------------')\n",
    "print(blending(data_set, c=100, g=10, ep=0, Kernel=\"linear\")[2])\n",
    "\n",
    "pred_1 = model.predict(x_test)\n",
    "pred_2 = model.predict(x_test)\n",
    "pred_3 = model.predict(x_test)\n",
    "\n",
    "LRclf = LinearRegression()\n",
    "SVR_clf = SVR(C=100, gamma=10, kernel=\"linear\", epsilon=0)\n",
    "Tree_clf = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "LRclf.fit(x_train, y_train)\n",
    "SVR_clf.fit(x_train, y_train)\n",
    "Tree_clf.fit(x_train, y_train)\n",
    "\n",
    "LR_pred = LRclf.predict(x_test)\n",
    "SVR_pred = SVR_clf.predict(x_test)\n",
    "Tree_pred = Tree_clf.predict(x_test)\n",
    "\n",
    "# 3行292列 それぞれの学習データを行報告に追加する\n",
    "total_pred = np.concatenate([[LR_pred, SVR_pred, Tree_pred]], axis=1)\n",
    "# それぞれの列で平均値を出力する\n",
    "pred_mean = np.mean(total_pred, axis=0)\n",
    "\n",
    "# mseを計算\n",
    "blending_mse = mean_squared_error(y_test,pred_mean)\n",
    "print(f'mse : blending_Data : {blending_mse}')\n",
    "\n",
    "# MSEが下がった手法\n",
    "# パターン１ それぞれの平均をとった（線形回帰・SVM・決定木）\n",
    "# パターン２ 対数変換を行なった（前処理）\n",
    "# パターン３ SVMのカーネルをrbf→linearに変更した\n"
   ]
  },
  {
   "source": [
    "## 【問題2】バギングのスクラッチ実装\n",
    "\n",
    "バギング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "バギングのベースとなるmse\n線形回帰 : 0.05207835114334287\nSVM      : 0.05393751032686638\n決定木   : 0.06602628200076659\nmse : bagging_mse : 0.36997561255868244\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LRclf = LinearRegression()\n",
    "SVR_clf = SVR(C=100, gamma=10, kernel=\"linear\", epsilon=0)\n",
    "Tree_clf = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "LRclf.fit(x_train, y_train)\n",
    "SVR_clf.fit(x_train, y_train)\n",
    "Tree_clf.fit(x_train, y_train)\n",
    "\n",
    "LR_pred = LRclf.predict(x_test)\n",
    "SVR_pred = SVR_clf.predict(x_test)\n",
    "Tree_pred = Tree_clf.predict(x_test)\n",
    "\n",
    "# 3行292列 それぞれの学習データを行報告に追加する\n",
    "total_pred = np.concatenate([[LR_pred, SVR_pred, Tree_pred]], axis=1)\n",
    "# それぞれの列で平均値を出力する\n",
    "pred_mean = np.mean(total_pred, axis=0)\n",
    "\n",
    "# mseを計算\n",
    "blending_mse = mean_squared_error(y_test,pred_mean)\n",
    "# print(f'mse : blending_Data : {blending_mse}')\n",
    "\n",
    "# ベースの推定\n",
    "print('バギングのベースとなるmse')\n",
    "print(f'線形回帰 : {mean_squared_error(y_test,LR_pred)}')\n",
    "print(f'SVM      : {mean_squared_error(y_test,SVR_pred)}')\n",
    "print(f'決定木   : {mean_squared_error(y_test,Tree_pred)}')\n",
    "\n",
    "# dataを作り直す回数\n",
    "n = 20\n",
    "\n",
    "models = []\n",
    "\n",
    "for i in range(n):\n",
    "    # 訓練データをさらにランダムに分割\n",
    "    X_bagging, _X, y_bagging, _y = train_test_split(x_train, y_train)\n",
    "    # モデルの定義\n",
    "    model = LinearRegression()\n",
    "    # 学習\n",
    "    model.fit(X_bagging,y_bagging)\n",
    "    # モデルを記録\n",
    "    models.append(model)\n",
    "\n",
    "# 初期配列\n",
    "prediction = np.zeros(len(x_test))\n",
    "\n",
    "# ランダム分割によって学習したモデルで推定結果を出す\n",
    "for model in models:\n",
    "    _predict = model.predict(x_test)\n",
    "    prediction = np.append(prediction, _predict)#０配列の列方向に追加していく\n",
    "\n",
    "# それぞれの列で平均値を出力する\n",
    "prediction_bagging = prediction.reshape(21, 292)#列を整理する\n",
    "\n",
    "pred_mean_1 = np.mean(prediction_bagging, axis=0)#列（縦方向）の平均\n",
    "bagging_mse = mean_squared_error(y_test,pred_mean_1)#MSE\n",
    "print(f'mse : bagging_mse : {bagging_mse}')\n",
    "\n",
    "# 20回ランダムに分割を繰り返したデータの平均をとると大きくMSEは下がった\n"
   ]
  },
  {
   "source": [
    "## 【問題3】スタッキングのスクラッチ実装"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<function blending at 0x7fbf19f47ca0>\n",
      "mse : stk_mse : 0.046503593119692065\n"
     ]
    }
   ],
   "source": [
    "class Stacking():\n",
    "    \"\"\"\n",
    "    スタッキング\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth, splits, models):\n",
    "        \"\"\"コンストラクタ\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_depth : スタッキングの最大深さ\n",
    "        splits : データ分割数\n",
    "        models : 使用モデル一覧\n",
    "        fit_models : 学習済みモデル保存用リスト\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.n_splits = splits\n",
    "        self.models = models\n",
    "        self.fit_models = []\n",
    "        \n",
    "    def calc_blending(self,x,y,model):\n",
    "        \"\"\"ブレンドデータ生成\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 説明変数\n",
    "        y : 目的変数\n",
    "        m : モデル\n",
    "        \"\"\"\n",
    "        # blendingの初期化\n",
    "        blending = np.zeros(len(x))\n",
    "        \n",
    "        # データの分割\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=False)\n",
    " \n",
    "        # 分割されたインデックスでループ\n",
    "        for train_index, test_index in kf.split(x):\n",
    "            # データ分割\n",
    "            x_train, x_test = x[train_index], x[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            y_train = y_train.ravel()\n",
    "            y_test = y_test.ravel()\n",
    "\n",
    "            # 学習\n",
    "            model.fit(x_train, y_train)\n",
    "            # 学習済みモデルの保存\n",
    "            self.fit_models.append(model)\n",
    "            # テストデータのインデックスに予測値を格納\n",
    "            blending[test_index] = model.predict(x_test)\n",
    "\n",
    "        return blending\n",
    "\n",
    "    print(blending)\n",
    "    \n",
    "    def fit(self,x,y,depth):\n",
    "        \"\"\"該当深さの学習実行\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 説明変数\n",
    "        y : 目的変数\n",
    "        depth : スタッキングの「現在の」深さ\n",
    "        \"\"\"\n",
    "        # スタッキングの「現在の」深さをメンバ変数化\n",
    "        self.depth=depth\n",
    "        \n",
    "        # 最大深さまで到達していれば、その深さのモデルを学習させて処理終了\n",
    "        if self.depth == self.max_depth:\n",
    "            # 当該深さでのモデルを取得\n",
    "            self.model = self.models[self.depth]\n",
    "            # 学習\n",
    "            self.model.fit(x,y)\n",
    "            return\n",
    "        \n",
    "        # 当該深さでのモデルを取得\n",
    "        models = self.models[self.depth]\n",
    "        self.blending = np.zeros([len(x),len(models)])\n",
    "        #1168*3\n",
    "        # print(self.blending.shape)\n",
    "        \n",
    "        # この階層の全てのモデルで学習\n",
    "        for i,model in enumerate(models):\n",
    "            _blending = self.calc_blending(x, y, model)\n",
    "            # print(_blending.shape)\n",
    "            self.blending[:,i] = _blending\n",
    "            # xxxxxxxxXXXXXXXXXXXXXXXXXXXXXX = _blending\n",
    "\n",
    "        # 再帰学習\n",
    "        # コンストラクタ生成の際の引数は同じ\n",
    "        self.stk = Stacking(self.max_depth, self.n_splits, self.models)\n",
    "        \n",
    "        # 学習実行の際は、ブレンドデータを説明変数として渡す。深さも1加えてやる\n",
    "        self.stk.fit(self.blending,y,depth+1)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        \"\"\"予測\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 説明変数\n",
    "        y : 目的変数\n",
    "        depth : スタッキングの「現在の」深さ\n",
    "        \"\"\"\n",
    "        # 最大深さの場合は最終的な予測値を出力するのみ\n",
    "        if self.depth == self.max_depth:\n",
    "            # 予測\n",
    "            prediction = self.model.predict(x)\n",
    "            # 返す\n",
    "            return prediction\n",
    "        # 最大深さに達していない場合は、再帰的に呼び出す\n",
    "        else:\n",
    "            \n",
    "            # 予測値を0で初期化\n",
    "            self.prediction = np.zeros(len(x))\n",
    "            # 次の階層に渡すブレンドデータ(仮)の作成\n",
    "            self.blending = np.zeros([len(x),len(self.models[self.depth])])\n",
    "            # この階層の学習済みモデルでループ\n",
    "            count = 0 # 現在どのモデルを回しているか把握\n",
    "            for model in self.fit_models:\n",
    "                # 予測し、0で初期化している予測値に加算\n",
    "                self.prediction += model.predict(x)\n",
    "                # 1種類のモデル学習の終了判定\n",
    "                count+=1\n",
    "                if count%self.n_splits == 0:\n",
    "                    # 予測値を加算してきたので割って平均値を算出\n",
    "                    self.prediction = self.prediction/self.n_splits\n",
    "                    # その平均値を次の階層で使用する説明変数に格納\n",
    "                    self.blending[:,int(count/self.n_splits)-1] = self.prediction\n",
    "                    # 次の種類のモデルでの予測のため、予測値は初期化しておく\n",
    "                    self.prediction = np.zeros(len(x))\n",
    "            # 次の階層の予測関数\n",
    "            prediction = self.stk.predict(self.blending)\n",
    "            return prediction\n",
    "\n",
    "models = {\n",
    "    0:[LinearRegression(),DecisionTreeRegressor(),RandomForestRegressor()],\n",
    "    1:[ARDRegression(),SGDRegressor(),DecisionTreeRegressor()],\n",
    "    2:[HuberRegressor(),ARDRegression(),RandomForestRegressor()],\n",
    "    3:LinearRegression()\n",
    "}\n",
    "\n",
    "stk = Stacking(max_depth=3,splits=5,models=models)\n",
    "stk.fit(x_train,y_train,0)\n",
    "\n",
    "stk_prediction = stk.predict(x_test)\n",
    "# print(stk_prediction)\n",
    "stk_mse = mean_squared_error(y_test,stk_prediction)#MSE\n",
    "print(f'mse : stk_mse : {stk_mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スタッキングも単独のモデルよりは精度が上がったが、バギングの方が精度の上昇が大きかった"
   ]
  }
 ]
}